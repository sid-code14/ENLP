# LLM-Based Summarization Evaluation
This repository is based on [paper](https://arxiv.org/abs/2303.16634) G-Eval and contains code for evaluating text summaries using open source Large Language Model (LLM) from HuggingFace. It aims to compare LLM-based evaluators to Human Ratings which uses a reference-based approach to assess the quality of system-generated summaries against reference texts.
## Overview
This project leverages LLM to evaluate quality of summaries based on SummEval dataset. It uses 4-bit quantization to optimize memory usage while maintaining model performance. The evaluation process loads reference texts and system-generated summaries, prompts the LLM to evaluate them, and stores the results.
## Dataset: SummEval
SummEval is a benchmark dataset for evaluating automatic text summarization systems using human judgments. It contains summaries generated by multiple models and includes human annotations on four key aspects: coherence, consistency, fluency, and relevance. The dataset also provides gold-standard reference summaries and scores from common automatic evaluation metrics like ROUGE and BERTScore. Based on CNN/Daily Mail articles, SummEval helps assess how well automatic metrics align with human evaluations, making it a valuable resource for benchmarking summarization models and improving evaluation methodologies. For more details, refer to the [paper](https://arxiv.org/abs/2007.12626)
## Requirements
````
Python 3.8+
PyTorch
Transformers
tqdm
Google Colab (for Google Drive integration)
Hugging Face account with access to Meta-Llama-3 models
````

## Setup

Set up your Hugging Face access token as an environment variable:
````
export HF_AUTH="your_huggingface_token"
````
Store your evaluation prompt in Drive at the location specified in the config file.\
Prepare your SummEval dataset JSON file and store it in drive.

## Project Structure
enlp.py: The main script that orchestrates the evaluation process.\
config.py: Configuration settings for the model, file paths, and generation parameters.

## Configuration
The config.py file contains important parameters:
### Model Configuration
````
MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct" # add any model from HuggingFace
HF_AUTH = os.environ.get('HF_AUTH')
````
### File Paths
````
ARGS = {
    'prompt_fp': 'Your path to prompt',
    'save_fp': 'Your path to save responses',
    'summeval_fp': 'Your path to SummEval Dataset',
}
````
### Generation Parameters
````
GENERATION_CONFIG = {
    'max_new_tokens': 128,
    'do_sample': True,
    'temperature': 0.1,
    'top_p': 0.9,
}
````
## Usage

Update the file paths in config.py to point to your specific files.\
Run the main script:
````
python enlp.py
````
The script will:

Load the HuggingFace model with 4-bit quantization.\
Process each summary in the SummEval dataset.\
Generate evaluations using the model.\
Save the results to the specified output file\



## How It Works

### Tokenizer and Model Loading 
The code initializes the tokenizer and model with 4-bit quantization for memory efficiency.\
### Data Processing 
For each instance in the SummEval dataset:\
The reference text and system-generated summary are extracted.\
The evaluation prompt template is filled with these texts.\
The prompt is tokenized and passed to the model

### Evaluation Generation 
The model generates an evaluation based on the prompt, with parameters controlled by `GENERATION_CONFIG`.
### Result Storage 
The results, including the prompt and model responses, are saved in a `JSON` file.

## Customization

Modify the prompt template to evaluate different aspects of summaries.\
Adjust generation parameters in `GENERATION_CONFIG` to control response length and diversity.\
Change the model by updating `MODEL_ID` to use different LLMs.\

## Memory Optimization
The code uses BitsAndBytes 4-bit quantization to reduce memory usage:
````
pythonCopybits_and_bytes_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)
````
## Output Format
The output JSON file contains the original SummEval instances enhanced with:

The complete evaluation prompt
The model's evaluation response

## Limitations

The evaluation quality depends on the prompt design and model capabilities.\
The 4-bit quantization may slightly impact model performance.\
The 8B parameter model may not be as comprehensive as larger models.
